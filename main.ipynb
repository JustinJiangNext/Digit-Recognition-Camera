{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinjiang/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "#config \n",
    "\n",
    "\n",
    "def fetchDevice(deviceName = \"Default\", logging = False) -> torch.device:\n",
    "    device:torch.device = None\n",
    "    if deviceName == \"Default\":\n",
    "        if (torch.backends.mps.is_available()):\n",
    "            device = torch.device('mps')\n",
    "            if logging:\n",
    "                print(\"Metal Performance Shaders Available! \")\n",
    "        elif(torch.cuda.is_available()):\n",
    "            device = torch.device('cuda')\n",
    "            if logging: \n",
    "                print(\"NVIDIA CUDA Available! \")\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "            if logging:\n",
    "                print(\"ONLY CPU Available! \")\n",
    "\n",
    "    else:\n",
    "        device = torch.device(deviceName)\n",
    "\n",
    "    return device\n",
    "# get gpu \n",
    "DEVICE = fetchDevice()\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# Skip frame for inferencing(saves computing power)\n",
    "FRAME_SKIP = 5\n",
    "\n",
    "# \"cnn\" or \"densenet\"\n",
    "MODEL_TYPE = \"densenet\"   \n",
    "\n",
    "# train/webcam\n",
    "MODE = \"webcam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn():\n",
    "    class SimpleCNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, 3, 1), nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, 3, 1), nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(9216, 128), nn.ReLU(),\n",
    "                nn.Linear(128, 10)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "    return SimpleCNN()\n",
    "\n",
    "def get_densenet():\n",
    "    model = models.densenet121(pretrained=False, num_classes=10)\n",
    "    model.features.conv0 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.features.pool0 = nn.Identity()\n",
    "    return model\n",
    "\n",
    "MODEL_PATHS = {\"cnn\": \"cnn_mnist.pth\", \"densenet\": \"densenet_mnist.pth\"}\n",
    "MODELS = {\"cnn\": get_cnn, \"densenet\": get_densenet}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_mnist.pth already exists. Skipping training.\n",
      "densenet_mnist.pth already exists. Skipping training.\n"
     ]
    }
   ],
   "source": [
    "# Only needed if neither model weight exists\n",
    "def train_and_save(model_type):\n",
    "    epochs = 5 if model_type == \"cnn\" else 3\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "    model = MODELS[model_type]().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(epochs):  # Few epochs for quick demo\n",
    "        model.train()\n",
    "        total, correct = 0, 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(imgs)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            preds = output.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        print(f\"[{model_type}] Epoch {epoch+1} train acc: {correct/total:.4f}\")\n",
    "    torch.save(model.state_dict(), MODEL_PATHS[model_type])\n",
    "    print(f\"{model_type} model saved to {MODEL_PATHS[model_type]}\")\n",
    "\n",
    "# Only train if file does not exist\n",
    "for mt in [\"cnn\", \"densenet\"]:\n",
    "    if not os.path.exists(MODEL_PATHS[mt]):\n",
    "        train_and_save(mt)\n",
    "    else:\n",
    "        print(f\"{MODEL_PATHS[mt]} already exists. Skipping training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinjiang/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/justinjiang/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "densenet model loaded from densenet_mnist.pth\n"
     ]
    }
   ],
   "source": [
    "# Select model based on MODEL_TYPE, always loads weights (never trains)\n",
    "model = MODELS[MODEL_TYPE]().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATHS[MODEL_TYPE], map_location=DEVICE))\n",
    "model.eval()\n",
    "print(f\"{MODEL_TYPE} model loaded from {MODEL_PATHS[MODEL_TYPE]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_digit(roi):\n",
    "    im = cv2.resize(roi, (28,28))\n",
    "    im = im.astype(np.float32) / 255.0\n",
    "    im = (im - 0.1307) / 0.3081\n",
    "    im = np.expand_dims(im, axis=0)  \n",
    "    return torch.from_numpy(im).unsqueeze(0).to(DEVICE)  # 1,1,28,28\n",
    "\n",
    "def find_digits(\n",
    "    frame_gray,\n",
    "    thresh_block_size=13,      # Must be odd, 11, 13, 15, 17...\n",
    "    thresh_C=15,                # Subtract from mean; higher = fewer detections (darker)\n",
    "    morph_ksize=5,             # Morph closing kernel size (helps connect strokes, fill gaps)\n",
    "    min_w=0, max_w=80,        # Min/max width of digit boxes in pixels\n",
    "    min_h=20, max_h=100,        # Min/max height of digit boxes in pixels\n",
    "    min_aspect=0.0, max_aspect=5.2,  # Width/height ratio: thin digits like \"1\" vs round like \"0\"\n",
    "    min_area=10                # Minimum contour area to be considered a digit\n",
    "):\n",
    "\n",
    "\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        frame_gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV,\n",
    "        thresh_block_size, thresh_C\n",
    "    )\n",
    "\n",
    "    if morph_ksize > 0:\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (morph_ksize, morph_ksize))\n",
    "        thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    digit_bboxes = []\n",
    "    for c in contours:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        area = cv2.contourArea(c)\n",
    "        aspect = w / float(h)\n",
    "        if (min_w <= w <= max_w and\n",
    "            min_h <= h <= max_h and\n",
    "            min_aspect <= aspect <= max_aspect and\n",
    "            area >= min_area):\n",
    "            digit_bboxes.append((x, y, w, h))\n",
    "    digit_bboxes = sorted(digit_bboxes, key=lambda b: b[0])\n",
    "    return digit_bboxes, thresh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press Q to quit.\n"
     ]
    }
   ],
   "source": [
    "if MODE == \"webcam\":\n",
    "    model.eval()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    # Force 480p resolution\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    frame_count = 0\n",
    "    last_digits = []\n",
    "\n",
    "    print(\"Press Q to quit.\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_disp = frame.copy()\n",
    "\n",
    "        if frame_count % FRAME_SKIP == 0:\n",
    "            digit_bboxes, thresh = find_digits(frame_gray)\n",
    "            digits = []\n",
    "            for bbox in digit_bboxes:\n",
    "                x, y, w, h = bbox\n",
    "                roi = thresh[y:y+h, x:x+w]\n",
    "                roi_pad = cv2.copyMakeBorder(roi, 10, 10, 10, 10, cv2.BORDER_CONSTANT, value=0)\n",
    "                im_tensor = preprocess_digit(roi_pad)\n",
    "                with torch.no_grad():\n",
    "                    pred = model(im_tensor)\n",
    "                    digit = pred.argmax(1).item()\n",
    "                digits.append((digit, x, y, w, h))\n",
    "            last_digits = digits  # update cache\n",
    "\n",
    "        # Always draw overlays from last_digits, even on skipped frames\n",
    "        for digit, x, y, w, h in last_digits:\n",
    "            cv2.rectangle(frame_disp, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame_disp, str(digit), (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "\n",
    "        frame_count += 1\n",
    "        cv2.imshow('Digit Recognition', frame_disp)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
